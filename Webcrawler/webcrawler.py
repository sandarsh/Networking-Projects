"""
--------------------------------------BEGIN-----------------------------------------
"""
""" 
Program Outline
------------------------------------------------------------------------------------
The main goal of this program is to implement a webcrawler which 
scrapes http://cs5700sp16.ccs.neu.edu/fakebook/ for hyperlinks 
and further crawls on the pages pointing to the domain 
cs5700sp16.ccs.neu.edu for more hyperlinks The crawler searches for five 
flags which are placed randomly on five of the web pages. The programs
halts its execution after the flags have been found.

Other objectives of this program were to:
—- Track the Frontier
—- Avoid loops while crawling
—- Only crawl the domain cscs5700sp16.ccs.neu.edu

This program is capable of handling - 
—- HTTP GET messages
—- HTTP POST messages
—- Cookies
—- Response codes 301, 302, 403, 404 and 500
------------------------------------------------------------------------------------


Function List for FakebookCrawler:
------------------------------------------------------------------------------------
establish_conn()
process_http_request()
login_to_fakebook()
get_request_status()
get_new_url()
handle_internal_server_error()
crawl()
find_flag()
openUrl()
urlExtractor()
------------------------------------------------------------------------------------
Design Strategy: Object Oriented Design
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
"""
import sys
import socket
import re
import urlparse
import string
from cgi import escape
from Queue import Queue
from bs4 import BeautifulSoup
"""
Class DEFINITION
------------------------------------------------------------------------------------
; FakebookCrawler: 
; The call to the constructor requires the USENAME and PASSWORD to be provided
; as input arguments from the command line while execution
; The crawler initializes the following definitions and data structures
;    --hostname      : "cs5700sp16.ccs.neu.edu"
;    --port          : 80
;    --username      : Passed as input argument
;    --password      : Passed as input argument
;    --csrftoken     : security token generated by fakebook
;    --session_id    : the session id generated for the session (Cookie)
;    --visited_links : The set of visited URLs
;    --out_links     : A list of URLs in the page being scraped currently
;    --flags_seen    : The list of flags found
;    --q             ; The queue of frontier URLs
"""
class FakebookCrawler(object):
    hostname = "cs5700sp16.ccs.neu.edu"
    port = 80

    def __init__(self, username, password):
        self.hostname = FakebookCrawler.hostname
        self.port = FakebookCrawler.port
        self.username = username
        self.password = password
        self.csrf_token = ''  # cookie's csrftoken
        self.session_id = ''  # cookie's sessionid
        self.visited_links= set()  # Used to avoid re-processing a page
        self.out_links = []
        self.flags_seen = []
    """
    FUNCTION DEFINITIONS
    ------------------------------------------------------------------------------------
    ; establish_conn: HOSTNAME(IP/URL) PORTNUM -> SOCKET
    ; GIVEN: A hostname (IP add or URL) and a port number as an argument
    ; RETURNS: A socket object with an active TCP session
    ; PURPOSE: To establish a TCP session with the remote host.
    : Examples: 
    ; establish_tcp_conn('cs5700sp16.ccs.neu.edu', 27993)
    ; establish_tcp_conn('129.10.113.143', 27993)
    """
    def establish_conn(self, host,port):
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((host, port))
        return s
    """
    ------------------------------------------------------------------------------------
    ; process_http_request: REQUESTDATA -> REPLYDATA
    ; GIVEN: A the raw request data to be sent
    ; RETURNS: The reply that was recevied after sending the data
    ; PURPOSE: To send the request and receive a reply
    : Examples: 
    ; process_http_request("GET ....") -> "200 O.K. ..."
    """
    def process_http_request(self, request):
        sock = self.establish_conn(self.hostname, self.port)
        sock.sendall(request)  
        reply = None
        reply = sock.recv(4096)
        sock.close() 
        return reply
    """
    ------------------------------------------------------------------------------------
    ; login_to_fakebook: SELF -> NULL
    ; GIVEN: A funtion call by the FakebookCrawler object
    ; RETURNS: Logs into fakebook
    ; PURPOSE: To log into fakebook
    """
    def login_to_fakebook(self):
        grequest = 'GET /accounts/login/ HTTP/1.1\r\nHost: %s\r\n\r\n' % self.hostname
        greply = self.process_http_request(grequest)
        session_string = re.compile(r'sessionid=([a-z0-9]+);')
        soup = BeautifulSoup(greply, "html.parser")
        self.csrf_token = soup.find('input', {'name':'csrfmiddlewaretoken'})['value']
        self.session_id = str(session_string.findall(greply)[0])
        url = str("%2F") + "fakebook" + str("%2f")
        pdata = 'username=%s&password=%s&csrfmiddlewaretoken=%s&?next=%s' % (self.username, self.password, self.csrf_token, url)
        prequest = 'POST /accounts/login/ HTTP/1.1\r\nConnection: Keep-Alive\r\nHost: %s\r\n' \
                          'Cookie: csrftoken=%s; sessionid=%s\r\nOrigin: %s\r\nContent-Length: 110\r\n' \
                          '\r\n%s\r\n\r\n' % (self.hostname, self.csrf_token, self.session_id, self.hostname, pdata)
        preply = self.process_http_request(prequest)
        self.session_id = str(session_string.findall(preply)[0])
     
    """
    ------------------------------------------------------------------------------------
    ; get_request_status: RESPONSEDATA -> RESPONSECODE
    ; GIVEN: The response data from an HTTP response
    ; RETURNS: The HTTP response code
    ; PURPOSE: To isolate the response code from the response
    : Examples: 
    ; get_request_status("200 O.K. ....") -> "200"
    """
    def get_request_status(self, content):
        i = string.find(content, ' ') + 1
        status = content[i: i + 3]
        return status
    """
    ------------------------------------------------------------------------------------
    ; get_new_url: RESPONSE -> NEWURL
    ; GIVEN: The data if the response code is 301 or 302
    ; RETURNS: The new URL in Location header
    ; PURPOSE: To isolate the new URL to redirect to
    : Examples: 
    ; get_new_url("302 Found .....Location: http://xyz.edu") -> http://xyz.edu
    """
    def get_new_url(self, content):
        new_loc = re.compile(r'Location=http://cs5700\.ccs\.neu\.edu(/fakebook/[a-z0-9/]+)')
        new_url = new_loc.findall(content)[0]
        return new_url
    """
    ------------------------------------------------------------------------------------
    ; handle_internal_server_error: URL -> CONTENT
    ; GIVEN: The URL which received the response 500
    ; RETURNS: The content after resendng the request until 200 is received
    ; PURPOSE: To retry in case of Internal Server Error (500)
    : Examples: 
    ; handle_internal_server_error("http://xyz.edu") -> "200 O.K. ..."
    """
    def handle_internal_server_error(self, url):
        status = None
        while status != '200':
            content = self.openUrl(url)
            status = self.get_request_status(content)
        return content
    """
    ------------------------------------------------------------------------------------
    ; crawl: SELF -> NULL
    ; GIVEN: A call by a FakebookCrawler Object
    ; RETURNS: NULL
    ; PURPOSE: To crawl Fakebook
    : Examples: 
    ; crawler.crawl() -> begins crawling fakebook until flags are found
    """
    def crawl(self):
        q = Queue()
        q.put("/fakebook/")
        while not q.empty() and len(self.flags_seen)<5:
            this_url = q.get()
            if this_url not in self.visited_links:
                page_content = self.openUrl(this_url)
                self.visited_links.add(this_url)
                status = self.get_request_status(page_content)
                if status == '200':
                    self.find_flag(page_content)
                    self.urlExtractor(page_content, this_url)
                    for link in self.out_links:
                        q.put(link)
                    continue
                elif status == '403' or status == '404': # Forbidden or Not Found:
                    self.visited_links.append(this_url)
                    continue
                elif status == '301' or status == '302': # Redirect to a new URL
                    this_url = self.get_new_url(page_content)
                    page_content = self.openUrl(this_url)
                elif status == '500': # Internal Server Error
                    page_content = self.handle_internal_server_error(this_url)
                self.find_flag(page_content)
                self.urlExtractor(page_content, this_url)
                for link in self.out_links:
                    q.put(link)
     """
    ------------------------------------------------------------------------------------
    ; find_flag: RESPONSECONTENT -> NULL
    ; GIVEN: The page content after http GET
    ; RETURNS: Prints the secret_flag if any on the page 
    ; PURPOSE: To find and print the secret flag
    : Examples: 
    ; find_flag("200 O.K. ... <h2 class="secret_flag" ..> xyz</h2>) -> print "xyz"
    """          
    def find_flag(self, content):
        soup = BeautifulSoup(content, "html.parser")
        flag_string = soup.find('h2', {'class': 'secret_flag'})
        if flag_string:
            flag_text = flag_string.get_text()
            flag = flag_text.split(' ')[1]
            print flag
            self.flags_seen.append(flag)

        else:
            pass   
    """
    ------------------------------------------------------------------------------------
    ; openUrl: URL -> RESPONSECONTENT
    ; GIVEN: A URL
    ; RETURNS: Build a GET message for the URL and sends it.
    ; PURPOSE: To send the GET message for the URL
    : Examples: 
    ; openUrl("http://xyz.edu") -> "200 O.K. ...."
    """
    def openUrl(self, url):
        request = 'GET %s HTTP/1.1\r\nHost: %s\r\nConnection: keep-alive\r\nCookie: csrftoken=%s; ' \
                  'sessionid=%s\r\n\r\n' % (url, self.hostname, self.csrf_token, self.session_id)
        content = self.process_http_request(request)
        return content
    """
    ------------------------------------------------------------------------------------
    ; urlExtractor: URL RESPONSECONTENT -> NULL
    ; GIVEN: A URL and the response from a GET
    ; RETURNS: NULL
    ; PURPOSE: Extracts all the hyperlinks from the given URL
    : Examples: 
    ; urlExtractor("200 O.K. .....") ->['<a href="abc.edu"> lmno </a>','...',...] 
    """
    def urlExtractor(self,content, url):
        if content:
            soup = BeautifulSoup(content, "html.parser")
            tags = soup('a')
            for tag in tags:
                href = tag.get("href")
                if href is not None:
                    url = urlparse.urljoin(url, escape(href))
                    if href.startswith("/fakebook/"):
                        self.out_links.append(url)
"""
PROGRAM BODY
------------------------------------------------------------------------------------
"""
if __name__ == "__main__":
    if (len(sys.argv) != 3):
        print 'Please check input command!'
        sys.exit()
    crawler = FakebookCrawler(sys.argv[1], sys.argv[2])
    crawler.login_to_fakebook()
    crawler.crawl()
"""
--------------------------------------END-------------------------------------------
"""